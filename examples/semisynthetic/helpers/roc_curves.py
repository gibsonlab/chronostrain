from pathlib import Path
from typing import List, Iterator, Tuple, Dict, Any
import argparse

import csv
import numpy as np
import pandas as pd
import torch

from chronostrain.config import create_logger
from chronostrain.database import StrainDatabase
from chronostrain import cfg

logger = create_logger("chronostrain.roc_curve")
device = torch.device("cuda:0")


def read_depth_dirs(base_dir: Path) -> Iterator[Tuple[int, Path]]:
    for child_dir in base_dir.glob("reads_*"):
        if not child_dir.is_dir():
            raise RuntimeError(f"Expected child `{child_dir}` to be a directory.")

        read_depth = int(child_dir.name.split("_")[1])
        yield read_depth, child_dir


def trial_dirs(read_depth_dir: Path) -> Iterator[Tuple[int, Path]]:
    for child_dir in read_depth_dir.glob("trial_*"):
        if not child_dir.is_dir():
            raise RuntimeError(f"Expected child `{child_dir}` to be a directory.")

        trial_num = int(child_dir.name.split("_")[1])
        yield trial_num, child_dir


def load_ground_truth(ground_truth_path: Path) -> pd.DataFrame:
    df_entries = []
    with open(ground_truth_path, 'r') as f:
        reader = csv.reader(f, delimiter=',', quotechar='\"')
        header_row = next(reader)

        assert header_row[0] == 'T'
        strain_ids = header_row[1:]
        for row in reader:
            t = float(row[0])
            for strain_id, abund in zip(strain_ids, row[1:]):
                abund = float(abund)
                df_entries.append({'T': t, 'Strain': strain_id, 'RelAbund': abund})
    return pd.DataFrame(df_entries)


def strip_suffixes(strain_id_string: str):
    suffixes = {'.chrom', '.fna', '.gz', '.bz', '.fastq', '.fasta'}
    x = Path(strain_id_string)
    while x.suffix in suffixes:
        x = x.with_suffix('')
    return x.name


def parse_chronostrain_estimate(db: StrainDatabase,
                                ground_truth: pd.DataFrame,
                                strain_ids: List[str],
                                output_dir: Path) -> torch.Tensor:
    abundance_samples = torch.load(output_dir / 'samples.pt')
    db_strains = [s.id for s in db.all_strains()]

    time_points = sorted(pd.unique(ground_truth['T']))
    if abundance_samples.shape[0] != len(time_points):
        raise RuntimeError("Number of time points ({}) in ground truth don't match sampled time points ({}).".format(
            len(time_points),
            abundance_samples.shape[0]
        ))

    if abundance_samples.shape[2] != len(db_strains):
        raise RuntimeError("Number of strains ({}) in database don't match sampled strain counts ({}).".format(
            len(db_strains),
            abundance_samples.shape[2]
        ))

    n_samples = abundance_samples.size(1)
    estimate = torch.zeros(size=(len(time_points), n_samples, len(strain_ids)), dtype=torch.float, device=device)
    strain_indices = {sid: i for i, sid in enumerate(strain_ids)}
    for db_idx, strain_id in enumerate(db_strains):
        s_idx = strain_indices[strain_id]
        estimate[:, :, s_idx] = abundance_samples[:, :, db_idx]
    return estimate


def parse_strainest_estimate(ground_truth: pd.DataFrame,
                             strain_ids: List[str],
                             sensitivity: str,
                             output_dir: Path) -> torch.Tensor:
    time_points = sorted(pd.unique(ground_truth['T']))
    strain_indices = {sid: i for i, sid in enumerate(strain_ids)}

    est_rel_abunds = torch.zeros(size=(len(time_points), len(strain_ids)), dtype=torch.float, device=device)
    for t_idx, t in enumerate(time_points):
        output_path = output_dir / f"abund_{t_idx}.{sensitivity}.txt"
        with open(output_path, 'rt') as f:
            lines = iter(f)
            header_line = next(lines)
            if not header_line.startswith('OTU'):
                raise RuntimeError(f"Unexpected format for file `{output_path}` generated by StrainEst.")

            for line in lines:
                strain_id, abund = line.rstrip().split('\t')
                strain_id = strip_suffixes(strain_id)
                abund = float(abund)
                try:
                    strain_idx = strain_indices[strain_id]
                    est_rel_abunds[t_idx][strain_idx] = abund
                except KeyError as e:
                    continue

    # Renormalize.
    row_sum = torch.sum(est_rel_abunds, dim=1, keepdim=True)
    support = torch.where(row_sum > 0)[0]
    zeros = torch.where(row_sum == 0)[0]
    est_rel_abunds[support, :] = est_rel_abunds[support, :] / row_sum[support]
    est_rel_abunds[zeros, :] = 1 / len(strain_ids)
    return est_rel_abunds


def extract_ground_truth_array(truth_df: pd.DataFrame, strain_ids: List[str]) -> torch.Tensor:
    time_points = sorted(pd.unique(truth_df['T']))
    t_idxs = {t: t_idx for t_idx, t in enumerate(time_points)}
    strain_idxs = {sid: i for i, sid in enumerate(strain_ids)}
    ground_truth = torch.zeros(size=(len(time_points), len(strain_ids)), dtype=torch.float, device=device)
    for _, row in truth_df.iterrows():
        s_idx = strain_idxs[row['Strain']]
        t_idx = t_idxs[row['T']]
        ground_truth[t_idx, s_idx] = row['RelAbund']
    return ground_truth


def df_entry(method_name: str, read_depth: int, trial_num: int, fpr: float, tpr: float) -> Dict[str, Any]:
    return {
        'ReadDepth': read_depth,
        'TrialNum': trial_num,
        'Method': method_name,
        'FPR': fpr,
        'TPR': tpr
    }


def chronostrain_roc(abundance_est: torch.Tensor, truth: torch.Tensor, strains: List[str]) -> Tuple[np.ndarray, np.ndarray]:
    lb = 1 / len(strains)
    quantiles = np.linspace(0, 1, 100)  # length Q
    abundance_est = abundance_est.cpu().numpy()  # T x N x S

    pred_indicators = np.quantile(abundance_est, quantiles, axis=1) > lb  # Q x T x S
    true_indicators = torch.gt(truth, torch.tensor(0.)).cpu().numpy()  # T x S

    true_positives = np.logical_and(pred_indicators, true_indicators)  # Q x T x S
    true_negatives = np.logical_and(np.logical_not(pred_indicators), np.logical_not(true_indicators))  # Q x T x S

    tpr = true_positives.sum(axis=-1).sum(axis=-1) / true_indicators.sum()
    tnr = true_negatives.sum(axis=-1).sum(axis=-1) / np.logical_not(true_indicators).sum()
    fpr = 1 - tnr
    return fpr, tpr


def strainest_roc(abundance_est: torch.Tensor, truth: torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:
    pred_indicators = torch.gt(abundance_est, torch.tensor(0.)).cpu().numpy()
    true_indicators = torch.gt(truth, torch.tensor(0.)).cpu().numpy()

    true_positives = np.logical_and(pred_indicators, true_indicators)  # Q x T x S
    true_negatives = np.logical_and(np.logical_not(pred_indicators), np.logical_not(true_indicators))  # Q x T x S

    tpr = true_positives.sum(axis=-1).sum(axis=-1) / true_indicators.sum()
    tnr = true_negatives.sum(axis=-1).sum(axis=-1) / np.logical_not(true_indicators).sum()
    fpr = 1 - tnr
    return fpr, tpr


def all_ecoli_strain_ids(index_path: Path) -> List[str]:
    df = pd.read_csv(index_path, sep='\t')
    return list(pd.unique(df.loc[
        (df['Genus'] == 'Escherichia') & (df['Species'] == 'coli'),
        'Accession'
    ]))


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument('-b', '--base_data_dir', type=str, required=True)
    parser.add_argument('-i', '--index_path', type=str, required=True)
    parser.add_argument('-o', '--out_dir', type=str, required=True)
    parser.add_argument('-g', '--ground_truth_path', type=str, required=True)
    return parser.parse_args()


def evaluate_sensitivities(index_df: pd.DataFrame,
                           ground_truth: pd.DataFrame,
                           result_base_dir: Path,
                           chronostrain_db: StrainDatabase) -> pd.DataFrame:
    strain_ids = [strain.id for strain in chronostrain_db.all_strains()]

    # search through all of the read depths.
    df_entries = []
    truth_tensor = extract_ground_truth_array(ground_truth, strain_ids)
    for read_depth, read_depth_dir in read_depth_dirs(result_base_dir):
        for trial_num, trial_dir in trial_dirs(read_depth_dir):
            logger.info(f"Handling read depth {read_depth}, trial {trial_num}")
            plot_dir = trial_dir / 'output' / 'plots'
            plot_dir.mkdir(exist_ok=True, parents=True)

            # =========== Chronostrain
            try:
                chronostrain_estimate_samples = parse_chronostrain_estimate(chronostrain_db, ground_truth, strain_ids,
                                                                            trial_dir / 'output' / 'chronostrain' / 'full_corr')
                fprs, tprs = chronostrain_roc(chronostrain_estimate_samples, truth_tensor, strain_ids)
                df_entries.append(df_entry('Chronostrain', read_depth, trial_num, 0.0, 0.0))
                df_entries.append(df_entry('Chronostrain', read_depth, trial_num, 1.0, 1.0))
                for fpr, tpr in zip(fprs, tprs):
                    df_entries.append(df_entry('Chronostrain', read_depth, trial_num, fpr, tpr))
            except FileNotFoundError:
                logger.info("Skipping Chronostrain output.")

            # =========== StrainEst (Sensitive)
            try:
                strainest_sens_estimate = parse_strainest_estimate(ground_truth, strain_ids,
                                                                   'sensitive',
                                                                   trial_dir / 'output' / 'strainest')
                strainest_estimate = parse_strainest_estimate(ground_truth, strain_ids,
                                                              'default',
                                                              trial_dir / 'output' / 'strainest')

                strainest_ests = torch.stack([strainest_sens_estimate, strainest_estimate])

                fprs, tprs = strainest_roc(strainest_ests, truth_tensor)
                df_entries.append(df_entry('StrainEst', read_depth, trial_num, 0.0, 0.0))
                df_entries.append(df_entry('StrainEst', read_depth, trial_num, 1.0, 1.0))
                for fpr, tpr in zip(fprs, tprs):
                    df_entries.append(df_entry('StrainEst', read_depth, trial_num, fpr, tpr))
            except FileNotFoundError as e:
                logger.info(f"Skipping StrainEst output (Couldn't find {e.filename})")
    df = pd.DataFrame(df_entries).groupby(['ReadDepth', 'TrialNum', 'Method', 'FPR']).max().reset_index()
    return df


def main():
    args = parse_args()
    result_base_dir = Path(args.base_data_dir)
    out_dir = Path(args.out_dir)

    # Load ground truth and refseq index.
    ground_truth = load_ground_truth(Path(args.ground_truth_path))
    index_df = pd.read_csv(args.index_path, sep='\t')
    chronostrain_db = cfg.database_cfg.get_database()
    out_dir.mkdir(exist_ok=True, parents=True)

    logger.info("Evaluating Sensitivity.")
    summary_df = evaluate_sensitivities(
        index_df,
        ground_truth,
        result_base_dir,
        chronostrain_db
    )
    out_path = out_dir / 'roc.csv'
    summary_df.to_csv(out_path, index=False)
    logger.info(f"[*] Saved ROC to {out_path}.")


if __name__ == "__main__":
    main()
