from pathlib import Path
from typing import List, Iterator, Tuple, Dict, Any
import argparse

import csv
import numpy as np
import pandas as pd
import torch

from chronostrain.database import StrainDatabase
from chronostrain.config import cfg
from chronostrain.logging import create_logger
logger = create_logger("chronostrain.roc_curve")
device = torch.device("cuda:0")


def read_depth_dirs(base_dir: Path) -> Iterator[Tuple[int, Path]]:
    for child_dir in base_dir.glob("reads_*"):
        if not child_dir.is_dir():
            raise RuntimeError(f"Expected child `{child_dir}` to be a directory.")

        read_depth = int(child_dir.name.split("_")[1])
        yield read_depth, child_dir


def trial_dirs(read_depth_dir: Path) -> Iterator[Tuple[int, Path]]:
    for child_dir in read_depth_dir.glob("trial_*"):
        if not child_dir.is_dir():
            raise RuntimeError(f"Expected child `{child_dir}` to be a directory.")

        trial_num = int(child_dir.name.split("_")[1])
        yield trial_num, child_dir


def load_ground_truth(ground_truth_path: Path) -> pd.DataFrame:
    df_entries = []
    with open(ground_truth_path, 'r') as f:
        reader = csv.reader(f, delimiter=',', quotechar='\"')
        header_row = next(reader)

        assert header_row[0] == 'T'
        strain_ids = header_row[1:]
        for row in reader:
            t = float(row[0])
            for strain_id, abund in zip(strain_ids, row[1:]):
                abund = float(abund)
                df_entries.append({'T': t, 'Strain': strain_id, 'RelAbund': abund})
    return pd.DataFrame(df_entries)


def strip_suffixes(strain_id_string: str):
    suffixes = {'.chrom', '.fna', '.gz', '.bz', '.fastq', '.fasta'}
    x = Path(strain_id_string)
    while x.suffix in suffixes:
        x = x.with_suffix('')
    return x.name


def parse_chronostrain_estimate(db: StrainDatabase,
                                ground_truth: pd.DataFrame,
                                strain_ids: List[str],
                                output_dir: Path) -> torch.Tensor:
    abundance_samples = torch.load(output_dir / 'samples.pt')
    db_strains = [s.id for s in db.all_strains()]

    time_points = sorted(pd.unique(ground_truth['T']))
    if abundance_samples.shape[0] != len(time_points):
        raise RuntimeError("Number of time points ({}) in ground truth don't match sampled time points ({}).".format(
            len(time_points),
            abundance_samples.shape[0]
        ))

    if abundance_samples.shape[2] != len(db_strains):
        raise RuntimeError("Number of strains ({}) in database don't match sampled strain counts ({}).".format(
            len(db_strains),
            abundance_samples.shape[2]
        ))

    n_samples = abundance_samples.size(1)
    estimate = torch.zeros(size=(len(time_points), n_samples, len(strain_ids)), dtype=torch.float, device=device)
    strain_indices = {sid: i for i, sid in enumerate(strain_ids)}
    for db_idx, strain_id in enumerate(db_strains):
        s_idx = strain_indices[strain_id]
        estimate[:, :, s_idx] = abundance_samples[:, :, db_idx]
    return estimate


def parse_strainest_estimate(ground_truth: pd.DataFrame,
                             strain_ids: List[str],
                             sensitivity: str,
                             output_dir: Path) -> torch.Tensor:
    time_points = sorted(pd.unique(ground_truth['T']))
    strain_indices = {sid: i for i, sid in enumerate(strain_ids)}

    est_rel_abunds = torch.zeros(size=(len(time_points), len(strain_ids)), dtype=torch.float, device=device)
    for t_idx, t in enumerate(time_points):
        output_path = output_dir / f"abund_{t_idx}.{sensitivity}.txt"
        with open(output_path, 'rt') as f:
            lines = iter(f)
            header_line = next(lines)
            if not header_line.startswith('OTU'):
                raise RuntimeError(f"Unexpected format for file `{output_path}` generated by StrainEst.")

            for line in lines:
                strain_id, abund = line.rstrip().split('\t')
                strain_id = strip_suffixes(strain_id)
                abund = float(abund)
                try:
                    strain_idx = strain_indices[strain_id]
                    est_rel_abunds[t_idx][strain_idx] = abund
                except KeyError as e:
                    continue

    # Renormalize.
    row_sum = torch.sum(est_rel_abunds, dim=1, keepdim=True)
    support = torch.where(row_sum > 0)[0]
    zeros = torch.where(row_sum == 0)[0]
    est_rel_abunds[support, :] = est_rel_abunds[support, :] / row_sum[support]
    est_rel_abunds[zeros, :] = 1 / len(strain_ids)
    return est_rel_abunds


def extract_ground_truth_array(truth_df: pd.DataFrame, strain_ids: List[str]) -> torch.Tensor:
    time_points = sorted(pd.unique(truth_df['T']))
    t_idxs = {t: t_idx for t_idx, t in enumerate(time_points)}
    strain_idxs = {sid: i for i, sid in enumerate(strain_ids)}
    ground_truth = torch.zeros(size=(len(time_points), len(strain_ids)), dtype=torch.float, device=device)
    for _, row in truth_df.iterrows():
        s_idx = strain_idxs[row['Strain']]
        t_idx = t_idxs[row['T']]
        ground_truth[t_idx, s_idx] = row['RelAbund']
    return ground_truth


def df_entry(method_name: str, read_depth: int, trial_num: int, fpr: float, tpr: float) -> Dict[str, Any]:
    return {
        'ReadDepth': read_depth,
        'TrialNum': trial_num,
        'Method': method_name,
        'FPR': fpr,
        'TPR': tpr
    }


def chronostrain_roc(abundance_est: torch.Tensor, truth: torch.Tensor, strains: List[str]) -> Tuple[np.ndarray, np.ndarray]:
    lb = 1 / len(strains)
    quantiles = np.linspace(0, 1, 100)  # length Q
    abundance_est = abundance_est.cpu().numpy()  # T x N x S

    pred_indicators = np.quantile(abundance_est, quantiles, axis=1) > lb  # Q x T x S
    true_indicators = torch.gt(truth, torch.tensor(0.)).cpu().numpy()  # T x S

    true_positives = np.logical_and(pred_indicators, true_indicators)  # Q x T x S
    true_negatives = np.logical_and(np.logical_not(pred_indicators), np.logical_not(true_indicators))  # Q x T x S

    tpr = true_positives.sum(axis=-1).sum(axis=-1) / true_indicators.sum()
    tnr = true_negatives.sum(axis=-1).sum(axis=-1) / np.logical_not(true_indicators).sum()
    fpr = 1 - tnr
    return fpr, tpr


def strainest_roc(abundance_est: torch.Tensor, truth: torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:
    pred_indicators = torch.gt(abundance_est, torch.tensor(0.)).cpu().numpy()
    true_indicators = torch.gt(truth, torch.tensor(0.)).cpu().numpy()

    true_positives = np.logical_and(pred_indicators, true_indicators)  # Q x T x S
    true_negatives = np.logical_and(np.logical_not(pred_indicators), np.logical_not(true_indicators))  # Q x T x S

    tpr = true_positives.sum(axis=-1).sum(axis=-1) / true_indicators.sum()
    tnr = true_negatives.sum(axis=-1).sum(axis=-1) / np.logical_not(true_indicators).sum()
    fpr = 1 - tnr
    return fpr, tpr


def all_ecoli_strain_ids(index_path: Path) -> List[str]:
    df = pd.read_csv(index_path, sep='\t')
    return list(pd.unique(df.loc[
        (df['Genus'] == 'Escherichia') & (df['Species'] == 'coli'),
        'Accession'
    ]))


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument('-b', '--base_data_dir', type=str, required=True)
    parser.add_argument('-i', '--index_path', type=str, required=True)
    parser.add_argument('-o', '--out_dir', type=str, required=True)
    parser.add_argument('-g', '--ground_truth_path', type=str, required=True)
    return parser.parse_args()


def evaluate_sensitivities(index_df: pd.DataFrame,
                           ground_truth: pd.DataFrame,
                           result_base_dir: Path,
                           chronostrain_db: StrainDatabase) -> pd.DataFrame:
    strain_ids = [strain.id for strain in chronostrain_db.all_strains()]

    # search through all of the read depths.
    df_entries = []
    truth_tensor = extract_ground_truth_array(ground_truth, strain_ids)
    for read_depth, read_depth_dir in read_depth_dirs(result_base_dir):
        for trial_num, trial_dir in trial_dirs(read_depth_dir):
            logger.info(f"Handling read depth {read_depth}, trial {trial_num}")
            plot_dir = trial_dir / 'output' / 'plots'
            plot_dir.mkdir(exist_ok=True, parents=True)

            # =========== Chronostrain
            try:
                chronostrain_estimate_samples = parse_chronostrain_estimate(chronostrain_db, ground_truth, strain_ids,
                                                                            trial_dir / 'output' / 'chronostrain' / 'full_corr')
                fprs, tprs = chronostrain_roc(chronostrain_estimate_samples, truth_tensor, strain_ids)
                df_entries.append(df_entry('Chronostrain', read_depth, trial_num, 0.0, 0.0))
                df_entries.append(df_entry('Chronostrain', read_depth, trial_num, 1.0, 1.0))
                for fpr, tpr in zip(fprs, tprs):
                    df_entries.append(df_entry('Chronostrain', read_depth, trial_num, fpr, tpr))
            except FileNotFoundError:
                logger.info("Skipping Chronostrain output.")

            # =========== StrainEst (Sensitive)
            try:
                strainest_sens_estimate = parse_strainest_estimate(ground_truth, strain_ids,
                                                                   'sensitive',
                                                                   trial_dir / 'output' / 'strainest')
                strainest_estimate = parse_strainest_estimate(ground_truth, strain_ids,
                                                              'default',
                                                              trial_dir / 'output' / 'strainest')

                strainest_ests = torch.stack([strainest_sens_estimate, strainest_estimate])

                fprs, tprs = strainest_roc(strainest_ests, truth_tensor)
                df_entries.append(df_entry('StrainEst', read_depth, trial_num, 0.0, 0.0))
                df_entries.append(df_entry('StrainEst', read_depth, trial_num, 1.0, 1.0))
                for fpr, tpr in zip(fprs, tprs):
                    df_entries.append(df_entry('StrainEst', read_depth, trial_num, fpr, tpr))
            except FileNotFoundError as e:
                logger.info(f"Skipping StrainEst output (Couldn't find {e.filename})")
    df = pd.DataFrame(df_entries).groupby(['ReadDepth', 'TrialNum', 'Method', 'FPR']).max().reset_index()
    return df


def main():
    args = parse_args()
    result_base_dir = Path(args.base_data_dir)
    out_dir = Path(args.out_dir)

    # Load ground truth and refseq index.
    ground_truth = load_ground_truth(Path(args.ground_truth_path))
    index_df = pd.read_csv(args.index_path, sep='\t')
    chronostrain_db = cfg.database_cfg.get_database()
    out_dir.mkdir(exist_ok=True, parents=True)

    logger.info("Evaluating Sensitivity.")
    summary_df = evaluate_sensitivities(
        index_df,
        ground_truth,
        result_base_dir,
        chronostrain_db
    )
    out_path = out_dir / 'roc.csv'
    summary_df.to_csv(out_path, index=False)
    logger.info(f"[*] Saved ROC to {out_path}.")


if __name__ == "__main__":
    main()
