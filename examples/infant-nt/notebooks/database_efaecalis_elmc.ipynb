{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a07fb2ef7d1774e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Database pipeline for quantification of E.faecalis from metagenomic data\n",
    "\n",
    "## What's in this notebook?\n",
    "\n",
    "This notebook is a modification of `efaecalis.ipynb` to specifically use the following collection of isolates as a reference database:\n",
    "1. The ~2000 isolates from European samples of _E.faecalis_\n",
    "2. The ~350 E. faecalis isolates from the ELMC infant analysis (https://www.nature.com/articles/s41467-022-35178-5)\n",
    "3. All other complete chromosomal assemblies from the Enterococcaceae family outside of _E. faecalis_.\n",
    "\n",
    "*Note: This notebook is specifically meant for running the `infant-nt` dataset analysis and thus we also include isolates from that study.*\n",
    "\n",
    "## Main Prerequisites\n",
    "\n",
    "We require the pre-downloaded collection of ~2000 isolates, archived a TSV file specified by path specified by `EUROPEAN_ISOLATE_INDEX`.\n",
    "We recommend spot-checking the isolates' fastQ files before archiving them into this index (e.g. using Kraken), and ensure that they truly are E. faecalis.\n",
    "\n",
    "This notebook also requires the index of infant isolates, downloaded using the script `infant_nt/download_assemblies.sh`.\n",
    "\n",
    "(Minor note: Note that we allow wildcards in the index file. For instance, you can split the contig multi-fasta into many files and specify a glob search; e.g. `<infant_id>/*.fasta`)\n",
    "\n",
    "All indices above must be a TSV file, with columns ((*) is required):\n",
    "1. Genus*\n",
    "2. Species*\n",
    "3. Strain name* (wrap with quotes if you must include whitespace)\n",
    "4. Accession* (a holdover from \"NCBI Accession\", but it is really just an ID column that just needs to be unique per row.)\n",
    "5. Assembly (For now, a metadata-only column, thus it is optional)\n",
    "6. SeqPath*\n",
    "7. ChromosomeLen (The length of the chromosome of this isolate/organism, or a best guess. This is metadata; convenient for post-hoc analyses for converting into sample-overall relabund.)\n",
    "8. GFF (Metadata, pointing to the location of the corresponding annotation, if one exists)\n",
    "   \n",
    "The column ordering is not strict -- but the column header names ARE. We use pandas `pd.read_csv` to read in a DataFrame which uses the column header.\n",
    "\n",
    "## Other prerequisites\n",
    "\n",
    "Other than that, the standard prereqs apply:\n",
    "We recommend using a `conda` environment for this notebook, with `ipywidgets` installed and updated. None of the operations of this notebook requires a GPU.\n",
    "This notebook requires that the following software is installed.\n",
    "- chronostrain (python>=3.10, the basic recipe `conda_basic.yml` or the full recipe `conda_full.yml`)\n",
    "- primersearch (http://emboss.open-bio.org/, https://anaconda.org/bioconda/emboss)\n",
    "- dashing2 (2023 Baker and Langmead: https://github.com/dnbaker/dashing2)\n",
    "\n",
    "### Hardware requirements\n",
    " \n",
    "None of the operations of this notebook requires a GPU. \n",
    "As of Aug 2023, we estimate that the contents of this notebook requires ~20 GB of hard disk space. \n",
    "At the time that we ran this pipeline, the catalog of non-isolate chromosomal assemblies totalled 14.3 GB, and isolates totalled 1.1GB.\n",
    "Other files (such as the BLAST database, marker seeds and chronostrain-specific byproducts) totalled 5.4 GB, with a peak of ~28 GB when accounting for temporary files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246f795-a67f-4524-8b24-b9962e04a908",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## File paths and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7a4e2f-7823-4e2a-b046-bbd897a65e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T13:57:12.606748999Z",
     "start_time": "2023-09-28T13:57:12.432061566Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "\"\"\" ============================================ EDIT THESE SETTINGS BASED ON USER'S CHOICE. ============================================ \"\"\"\n",
    "\"\"\" RefSeq catalog settings\"\"\"\n",
    "TARGET_DIR = Path(\"/mnt/e/infant_nt/database\")  # the base directory for everything else.\n",
    "\n",
    "# ========== for infant catalog. The infant index <INFANT_ISOLATE_INDEX> will be created using the infants <infant_id>/isolate_asemblies/metadata.tsv\n",
    "INFANT_CATALOG_DIR = Path(\"/mnt/e/infant_nt\")\n",
    "INFANT_ISOLATE_INDEX = TARGET_DIR / 'infant_isolates' / 'index.tsv'\n",
    "# ========== European isolate catalog of E. faecalis\n",
    "EUROPEAN_ISOLATE_INDEX = Path(\"/data/local/europe_efaecalis/index.tsv\")\n",
    "# ========== RefSeq index of Enterococcaceae (see efaecalis.ipynb. Download the index of chromosomal assemblies, then take the subset of all non-efaecalis genomes.)\n",
    "REFSEQ_INDEX = Path(\"/mnt/e/infant_nt/database/enterococcaceae_index.tsv\")\n",
    "\n",
    "\"\"\" RefSeq BLAST database \"\"\"\n",
    "BLAST_DB_DIR = TARGET_DIR / \"blast_db\"\n",
    "BLAST_DB_NAME = \"Efcs_Europe_ELMC\"  # Blast DB to create.\n",
    "\n",
    "\"\"\" Marker seeds \"\"\"\n",
    "MARKER_SEED_DIR = TARGET_DIR / \"marker_seeds\"\n",
    "MARKER_SEED_INDEX = MARKER_SEED_DIR / \"marker_seed_index.tsv\"\n",
    "\n",
    "\"\"\" chronostrain-specific settings \"\"\"\n",
    "NUM_CORES = 8  # number of cores to use (e.g. for blastn)\n",
    "MIN_PCT_IDTY = 75  # accept BLAST hits as markers above this threshold.\n",
    "CHRONOSTRAIN_DB_DIR = TARGET_DIR / \"chronostrain_files\"  # The directory to use for chronostrain's database files.\n",
    "CHRONOSTRAIN_TARGET_JSON = CHRONOSTRAIN_DB_DIR / \"efaecalis.json\"  # the desired final product.\n",
    "CHRONOSTRAIN_TARGET_CLUSTERS = CHRONOSTRAIN_DB_DIR / \"efaecalis.clusters.txt\"  # the clustering file.\n",
    "DASHING2_DIR = Path(\"/home/youn/work/bin\")  # Directory that contains the dashing2 executable.\n",
    "\n",
    "\n",
    "\"\"\" ============================================ DO NOT EDIT BELOW ============================================ \"\"\"\n",
    "\"\"\" environment variable extraction \"\"\"\n",
    "try:\n",
    "    VARS_SET\n",
    "except NameError:\n",
    "    VARS_SET = True\n",
    "    _cwd = %pwd\n",
    "    _DB_HELPER_DIR = Path(_cwd).parent.parent / 'database'\n",
    "    _start_path = %env PATH\n",
    "\n",
    "# Work in database example directory, where all the helper scripts and settings.sh are.\n",
    "%cd \"$_DB_HELPER_DIR\"\n",
    "# Don't use GPU when importing jaxlib through chronostrain.\n",
    "%env JAX_PLATFORM_NAME=cpu  \n",
    "%env TARGET_TAXA=$TARGET_TAXA\n",
    "%env NCBI_REFSEQ_DIR=$NCBI_REFSEQ_DIR\n",
    "%env REFSEQ_INDEX=$REFSEQ_INDEX\n",
    "# Need basic executables, such as \"which\" and \"basename\" (required by primersearch)\n",
    "%env PATH=/usr/bin:$_start_path:$DASHING2_DIR"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe9ac0c8-1267-4d81-a72d-43384889c37f",
   "metadata": {},
   "source": [
    "# === Ensure that these commands work.\n",
    "print(\"checking EMBOSS primersearch.\")\n",
    "!primersearch --version\n",
    "\n",
    "print(\"\\nchecking dashing2.\")\n",
    "!dashing2 --version\n",
    "\n",
    "print(\"\\nchecking pgap.\")\n",
    "!pgap --version"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "adda6630-d7c5-4bd7-ac66-e5d1fc8dbed8",
   "metadata": {},
   "source": [
    "## Recipe starts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f279e6f-8d09-4f7e-86f2-6d4e5eb0b478",
   "metadata": {},
   "source": [
    "# Prepare directories.\n",
    "TARGET_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"made {TARGET_DIR}\")\n",
    "BLAST_DB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"made {BLAST_DB_DIR}\")\n",
    "MARKER_SEED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"made {MARKER_SEED_DIR}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c5d3c0d8-296f-44f6-9f3e-c905f52d1adc",
   "metadata": {},
   "source": [
    "### Step 1 - include isolate assemblies\n",
    "\n",
    "Include infant isolates to the database catalog.\n",
    "\n",
    "*Note: This cell does nothing if `infant_nt/download_assemblies.sh` has not been run. It can be safely skipped if one does not want to include these isolates.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472b5cdf-dae7-4d30-90e5-feb9a1e0d436",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# this file points to the output of infant_nt/download_assembly_catalog.sh.\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "infant_isolate_df_entries = []\n",
    "for f in INFANT_CATALOG_DIR.glob(\"*/isolate_assemblies/metadata.tsv\"):\n",
    "    print(\"Found metadata in dir {}.\".format(f.parent))\n",
    "\n",
    "    # =============== Create a dataframe.\n",
    "    isolate_df = pd.read_csv(f, sep='\\t')\n",
    "    for _, row in isolate_df.iterrows():\n",
    "        # Parse entries.\n",
    "        infant_id = row['Participant']\n",
    "        acc = row['Accession']\n",
    "        genus = row['Genus']\n",
    "        species = row['Species']\n",
    "        timepoint = row['Timepoint']\n",
    "        sample_id = row['SampleId']\n",
    "        source_fasta_path = Path(row['FastaPath'])\n",
    "\n",
    "        # Skip if not E. faecalis.\n",
    "        if not (genus == 'Enterococcus' and species == 'faecalis'):\n",
    "            continue\n",
    "\n",
    "        # Extract the records.\n",
    "        records = list(SeqIO.parse(source_fasta_path, format=\"fasta\"))\n",
    "        total_contig_len = sum(len(record.seq) for record in records)\n",
    "\n",
    "        # Do nothing if no records are available.\n",
    "        if len(records) == 0:\n",
    "            continue\n",
    "\n",
    "        # Add to the dataframe.\n",
    "        infant_isolate_df_entries.append(\n",
    "            (genus, species, f'{infant_id}_t:{timepoint}_s:{sample_id}', infant_id, timepoint, acc, acc, str(source_fasta_path), total_contig_len, 'None')\n",
    "        )\n",
    "\n",
    "\n",
    "# Create dataframe and save to file.\n",
    "if len(infant_isolate_df_entries) > 0:\n",
    "    infant_isolate_df = pd.DataFrame(\n",
    "        infant_isolate_df_entries, \n",
    "        columns=['Genus', 'Species', 'Strain', 'Infant', 'T', 'Accession', 'Assembly', 'SeqPath', 'ChromosomeLen', 'GFF']\n",
    "    ).astype(\n",
    "        {\n",
    "            'Genus': 'string',\n",
    "            'Species': 'string',\n",
    "            'Strain': 'string',\n",
    "            'Infant': 'string',\n",
    "            'T': 'string',\n",
    "            'Accession': 'string',\n",
    "            'Assembly': 'string',\n",
    "            'SeqPath': 'string',\n",
    "            'ChromosomeLen': 'int',\n",
    "            'GFF': 'string'\n",
    "        }\n",
    "    )\n",
    "    infant_isolate_df.to_csv(INFANT_ISOLATE_INDEX, sep='\\t', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dbef23e0-cf03-4139-adcd-527b932c0026",
   "metadata": {},
   "source": [
    "### Step 2: Build the marker seed catalog.\n",
    "\n",
    "ChronoStrain only needs a FASTA file of marker seeds (one multi-fasta file per marker gene), and a single TSV file that catalogs them.\n",
    "However, to get there, we need to take a few steps...\n",
    "\n",
    "#### Step 2.1: Download MLST schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "053f68f2-641e-4a38-866e-9ba5b9fcbd19",
   "metadata": {},
   "source": [
    "!python python_helpers/mlst_download.py -t \"Enterococcus faecalis\" -w \"$MARKER_SEED_DIR\"/mlst_schema -o \"$MARKER_SEED_DIR\"/mlst_seeds.tsv"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d382c6-2e7a-4579-8a74-22fc1f2e37a3",
   "metadata": {},
   "source": [
    "#### Step 2.2: Non-standard genes\n",
    "\n",
    "Here, we save some work and re-use the marker seeds discovered using PCR primers in the `database` example notebook (efaecalis.ipynb)\n",
    "\n",
    "(The FASTA files representing primer-hits with lengths <150 were left out. We also took the first entry for each fasta file (due to the primer hits' overall similarity) except for fsrB which appears to have quite a bit of variability.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ab860e5-fac0-405a-acff-4a7b2aff10f4",
   "metadata": {},
   "source": [
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cbh.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsA.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsB.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsC.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsD.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsE.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsF.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsG.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsH.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsI.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsJ.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cpsK.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cylA.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/cylB.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/esp.fasta \"$MARKER_SEED_DIR\"/\n",
    "# !cp /mnt/e/infant_nt/database/marker_seeds_efaecalis_original/fsrB.fasta \"$MARKER_SEED_DIR\"/\n",
    "\n",
    "with open(MARKER_SEED_DIR / \"manual_seeds.tsv\", \"wt\") as metadata_tsv:\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cbh\", MARKER_SEED_DIR / \"cbh.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsA\", MARKER_SEED_DIR / \"cpsA.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsB\", MARKER_SEED_DIR / \"cpsB.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsC\", MARKER_SEED_DIR / \"cpsC.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsD\", MARKER_SEED_DIR / \"cpsD.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsE\", MARKER_SEED_DIR / \"cpsE.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsF\", MARKER_SEED_DIR / \"cpsF.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsG\", MARKER_SEED_DIR / \"cpsG.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsH\", MARKER_SEED_DIR / \"cpsH.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsI\", MARKER_SEED_DIR / \"cpsI.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsJ\", MARKER_SEED_DIR / \"cpsJ.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cpsK\", MARKER_SEED_DIR / \"cpsK.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cylA\", MARKER_SEED_DIR / \"cylA.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"cylB\", MARKER_SEED_DIR / \"cylB.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"esp\", MARKER_SEED_DIR / \"esp.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)\n",
    "    print(\"{}\\t{}\\t{}\".format(\"fsrB\", MARKER_SEED_DIR / \"fsrB.fasta\", \"POLYMORPHIC\"), file=metadata_tsv)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5175fe89-5795-425b-99e2-82e5ef6bfb33",
   "metadata": {},
   "source": [
    "#### Step 2.3: Annotate isolates.\n",
    "\n",
    "**Note 1: The ELMC isolates are grabbed via the `infant-nt` example (`download_assemblies.sh`)**\n",
    "\n",
    "**Note 2: this step takes a long time; consider assigning the task to a dedicated compute cluster.** Alternatively, grab the pre-computed annotation GFF files from (Todo zenodo record URL)**, and place its contents into <INFANT_ISOLATE_INDEX.parent>, so that it matches the directory pattern `<INFANT_ISOLATE_INDEX.parent>/annotations/<isolate_acc>`.\n",
    "\n",
    "The cell has been included anyway, with the code commented out to demonstrate how it was done within the context of this notebook.\n",
    "\n",
    "For clarity: The goal here is to run the `pgap` tool (Prokaryotic Genome Annotation Pipeline) from NCBI to annotate the genome of each isolate, via the command\n",
    "`pgap -r -o <INFANT_ISOLATE_INDEX.parent>/annotations/<isolate_acc> -g <isolate_assembly_fasta> -s 'Enterococcus faecalis'`\n",
    "where the fasta file `<isolate_assembly_fasta>` points to a multi-fasta file of the assembled contigs. \n",
    "\n",
    "The only catch is that `pgap` does not handle fasta records with malformed IDs, has a sequence of trailing N's on either ends, nor records with length < 200. Thus N's must be trimmed from each record and records of length < 200 (after trimming) must be removed.\n",
    "([Reference: pgap github wiki](https://github.com/ncbi/pgap/wiki/Input-Files))\n",
    "The isolate FASTA files have plenty of records that violate these rules, so some cleaning must be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5712c60-49be-4271-af67-38afd280b34b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "#### Uncomment the code in this cell to run it within the notebok. This requires NCBI's pgap software.\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "# cwd = Path().resolve()\n",
    "\n",
    "# print(f\"Reading infant isolate catalog from {INFANT_CATALOG_DIR}/*/isolate_assemblies/metadata.tsv\")\n",
    "# for f in INFANT_CATALOG_DIR.glob(\"*/isolate_assemblies/metadata.tsv\"):\n",
    "#     infant_id = f.parent.parent.name\n",
    "#     print(f\"Handling isolates from {infant_id}\")\n",
    "    \n",
    "#     # =============== Create a dataframe.\n",
    "#     isolate_df = pd.read_csv(f, sep='\\t')\n",
    "#     for _, row in isolate_df.iterrows():\n",
    "#         # Parse entries.\n",
    "#         participant = row['Participant']\n",
    "#         acc = row['Accession']\n",
    "#         genus = row['Genus']\n",
    "#         species = row['Species']\n",
    "#         timepoint = row['Timepoint']\n",
    "#         sample_id = row['SampleId']\n",
    "#         source_fasta_path = Path(row['FastaPath'])\n",
    "\n",
    "#         # Skip if not E. faecalis.\n",
    "#         if not (genus == 'Enterococcus' and species == 'faecalis'):\n",
    "#             continue\n",
    "\n",
    "#         # setup\n",
    "#         isolate_out_dir = INFANT_ISOLATE_INDEX.parent / \"annotations\" / acc\n",
    "#         tmp_dir = isolate_out_dir.parent / '_tmp'\n",
    "#         taxa_name = f'{genus} {species}'\n",
    "\n",
    "#         isolate_out_dir.parent.mkdir(exist_ok=True, parents=True)\n",
    "#         tmp_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "#         # reformat fasta file, remove punctuations.\n",
    "#         fixed_fasta_path = tmp_dir / f\"{acc}.fasta\"\n",
    "#         with open(fixed_fasta_path, \"wt\") as f:\n",
    "#             for r_idx, record in enumerate(SeqIO.parse(source_fasta_path, \"fasta\")):\n",
    "#                 seq = record.seq.rstrip('N')\n",
    "#                 if len(seq) < 200:\n",
    "#                     l = len(record.seq)\n",
    "#                     print(f\"Skipping record {record.id} because it has length < 200 (len={l}) (possibly after stripping Ns)\")\n",
    "#                     continue\n",
    "#                 record.id = f\"{acc}:CONTIG_{r_idx}\"\n",
    "#                 record.seq = seq\n",
    "#                 SeqIO.write([record], handle=f, format=\"fasta\")\n",
    "\n",
    "#         # run pgap.\n",
    "#         os.chdir(tmp_dir)\n",
    "#         print(f\"[DIR = {tmp_dir}] pgap -r -o {isolate_out_dir} -g {fixed_fasta_path} -s '{taxa_name}'\")\n",
    "#         !pgap -r -o {isolate_out_dir} -g {fixed_fasta_path} -s '{taxa_name}' --quiet --no-self-update\n",
    "        \n",
    "#         # go back to working dir, and cleanup.\n",
    "#         os.chdir(cwd)\n",
    "#         shutil.rmtree(tmp_dir)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f6087367-dfbf-4d16-86ba-3d7117115c42",
   "metadata": {},
   "source": [
    "Next, parse the annotations and grab the genes. The cell below requires the package GFF package (https://github.com/chapmanb/bcbb/tree/master/gff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b751b901-0ec1-41c6-ac39-816f00bef446",
   "metadata": {},
   "source": [
    "import string\n",
    "from BCBio import GFF\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# ================ Functions for appending gene sequences to fasta records\n",
    "gene_paths = {}\n",
    "def get_gene_path(gene_name):\n",
    "    \"\"\"\n",
    "    Retrieves the gene path from the dist `gene_paths`, but with some pre-cleaning.\n",
    "    \"\"\"\n",
    "    if gene_name in gene_paths:\n",
    "        return gene_paths[gene_name]\n",
    "    else:\n",
    "        p = INFANT_ISOLATE_INDEX.parent / 'genes' / f'{gene_name}.fasta'\n",
    "        if p.exists():\n",
    "            p.unlink()  # clear the contents\n",
    "        else:\n",
    "            p.parent.mkdir(exist_ok=True, parents=True)\n",
    "        gene_paths[gene_name] = p\n",
    "        return p\n",
    "\n",
    "\n",
    "def add_gene_seq(gene_name: str, gene_seq: Seq, gene_id: str, description: str = \"\"):\n",
    "    \"\"\"\n",
    "    Adds a sequence (gene_seq) to a fasta file, where path is derived from get_gene_path.\n",
    "    \"\"\"\n",
    "    gene_path = get_gene_path(gene_name)\n",
    "    # this opens the fasta in \"a\" mode, but get_gene_path clears its contents to ensure that each time this code is run, it's a fresh start.\n",
    "    with open(gene_path, \"a\") as f:  \n",
    "        SeqIO.write(\n",
    "            [SeqRecord(gene_seq, id=gene_id, name=gene_name, description=description)], \n",
    "            handle=f, \n",
    "            format='fasta'\n",
    "        )\n",
    "\n",
    "\n",
    "def remove_punctuations(s: str) -> str:\n",
    "    \"\"\"\n",
    "    To ensure that a gene name can be translated into a POSIX-friendly file path, this function removes punctuations.\n",
    "    \"\"\"\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "\n",
    "# ========================= Process the pgap results, using the above helpers.\n",
    "df_entries = []\n",
    "\n",
    "# annot_dir = INFANT_ISOLATE_INDEX.parent / \"annotations\"\n",
    "annot_dir = Path(\"/data/cctm/youn/infant_nt/isolates\")  # testing; delete this later.\n",
    "infant_isolate_df = pd.read_csv(INFANT_ISOLATE_INDEX, sep='\\t')  # load the isolate catalog, which has metadata that we need\n",
    "\n",
    "pbar = tqdm(infant_isolate_df.iterrows(), total=infant_isolate_df.shape[0])\n",
    "for _, row in pbar:\n",
    "    acc = row['Accession']\n",
    "    pbar.set_postfix({\"Isolate\": acc})\n",
    "    annot_subdir = annot_dir / acc\n",
    "    gff_path = annot_subdir / \"annot.gff\"  # annotations file containing gene names and locations\n",
    "    seq_path = annot_subdir / f\"{acc}.fasta\"  # the sequence fasta file to extract genes from\n",
    "    if not gff_path.exists():\n",
    "        print(f\"annot.gff not found for {acc}. Did pgap run correctly?\")\n",
    "        continue\n",
    "\n",
    "    seq_records = {record.id: record.seq for record in SeqIO.parse(seq_path, \"fasta\")}\n",
    "    with open(gff_path, \"rt\") as gff_handle:\n",
    "        for rec in GFF.parse(gff_handle, limit_info=dict(gff_type=['gene'])):\n",
    "            # assume that the record is of the format (<isolate_acc>:CONTIG_<contig_index>), to be handled via the PGAP preprocessing. (see above cell)\n",
    "            contig_idx = int(rec.id.split(\"_\")[-1])\n",
    "            \n",
    "            for feature in rec.features:\n",
    "                # find gene name; exclude putative genes (pgaptmp_*)\n",
    "                gene_names = sorted([remove_punctuations(n) for n in feature.qualifiers['Name'] if not n.startswith('pgaptmp')])\n",
    "                \n",
    "                if len(gene_names) > 0:\n",
    "                    gene_name = gene_names[0]  # if there are multiple candidate names, take the lexicographically first one.\n",
    "                    loc = feature.location\n",
    "                    gene_seq = loc.extract(seq_records[rec.id])\n",
    "\n",
    "                    # Add count to dataframe\n",
    "                    df_entries.append({\n",
    "                        'Accession': acc,\n",
    "                        'Contig': contig_idx,\n",
    "                        'Gene': gene_name,\n",
    "                        'Length': len(gene_seq),\n",
    "                        'PositionLeft': int(loc.start),\n",
    "                        'PositionRight': int(loc.end),\n",
    "                        'PositiveStrand': loc.strand > 0\n",
    "                    })\n",
    "\n",
    "                    # write record to fasta\n",
    "                    gene_id = f\"{acc}:{gene_name}:{contig_idx}\"\n",
    "                    desc = f\"Extracted from {acc} using pgap, position {str(loc)} of contig index {contig_idx}\"\n",
    "                    add_gene_seq(gene_name, gene_seq, gene_id, description=desc)\n",
    "    \n",
    "    del seq_records  # clean up\n",
    "\n",
    "gene_annotations = pd.DataFrame(df_entries)\n",
    "del df_entries  # clean up\n",
    "\n",
    "gene_annotations.to_feather(INFANT_ISOLATE_INDEX.parent / \"genes.feather\")\n",
    "display(gene_annotations)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ca5d0f43-0af0-45bb-828c-21f70f95e572",
   "metadata": {},
   "source": [
    "Run multiple alignment. This step requires MAFFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48be6972-0083-4975-855b-2f9f3c658544",
   "metadata": {},
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gene_annotations = pd.read_feather(INFANT_ISOLATE_INDEX.parent / \"genes.feather\")\n",
    "n_isolate_accs = len(pd.unique(gene_annotations['Accession']))\n",
    "tol_frac = 0.5  # don't align genes found in fewer than this fraction of isolates\n",
    "\n",
    "n_genes = len(pd.unique(gene_annotations['Gene']))\n",
    "pbar = tqdm(gene_annotations.groupby(\"Gene\"), total=n_genes)\n",
    "\n",
    "\n",
    "for gene_name, gene_section in pbar:\n",
    "    pbar.set_postfix({\"Gene\": gene_name})\n",
    "    count = gene_section.shape[0]\n",
    "    if count < tol_frac * n_isolate_accs:\n",
    "        continue\n",
    "    \n",
    "    # run multiple alignment.\n",
    "    gene_path = gene_paths[gene_name]\n",
    "    aln_path = INFANT_ISOLATE_INDEX.parent / 'gene_alignments' / f'{gene_name}.aln.fasta'\n",
    "    if not aln_path.parent.exists():\n",
    "        aln_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    os.system(f\"mafft --nuc --quiet --thread 8 {gene_path} > {aln_path}\")  # \">\" overwrites any previous runs."
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eda3fcd7-a024-4509-aed0-53d2861d743e",
   "metadata": {},
   "source": [
    "Parse the alignments. Pick the genes that best separate the isolates that are clustered together using only the ST genes + polymorphism/virulence islands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac6037fc-89eb-4028-b031-b0ec88116e65",
   "metadata": {},
   "source": [
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "infant_isolate_df = pd.read_csv(INFANT_ISOLATE_INDEX, sep='\\t')  # load the isolate catalog, which has metadata that we need\n",
    "gene_annotations = pd.read_feather(INFANT_ISOLATE_INDEX.parent / \"genes.feather\")  # run this if previous cell already ran in a prior instantiation\n",
    "\n",
    "\n",
    "# a function that will compute the necessary distance matrix.\n",
    "def get_splitting_genes(infant_groupings: Dict[str, List[str]], infant_ordering: Dict[str, int]):\n",
    "    \"\"\"\n",
    "    Uses each gene's distane matrix to determine a collection of genes which \"distinguishes\" them across infants.\n",
    "    Refer to `load_gene_distance_matrix` for the details as to what the terms \"distance\" and \"distinguishes\" mean.\n",
    "    \"\"\"\n",
    "    union_isolates = set()\n",
    "    for _, isolate_list in infant_groupings.items():\n",
    "        union_isolates = union_isolates.union(set(isolate_list))\n",
    "        \n",
    "    gene_subset = gene_annotations.loc[gene_annotations['Accession'].isin(union_isolates)]\n",
    "    gene_names = []\n",
    "    gene_nnzs = []\n",
    "    for gene_name, gene_section in gene_subset.groupby(\"Gene\"):\n",
    "        # isolates_with_gene = set(gene_section['Accession'])\n",
    "        # isolates_without_gene = this_isolates.difference(isolates_with_gene)\n",
    "\n",
    "        try:\n",
    "            d = load_gene_distance_matrix(gene_name, infant_groupings, infant_ordering, union_isolates)  # shape is (n_infants, n_infants)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        d_tril = d[np.tril_indices(n=len(infant_ordering), k=-1)]\n",
    "        frac_nnz = np.sum(d_tril > 0) / len(d_tril)\n",
    "\n",
    "        gene_names.append(gene_name)\n",
    "        gene_nnzs.append(frac_nnz)\n",
    "    ordering = np.argsort(gene_nnzs)[::-1]  # decreasing order\n",
    "\n",
    "    return [\n",
    "        gene_names[k]\n",
    "        for k in ordering[:3]  # top one\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_gene_distance_matrix(gene_name: str, infant_groupings: Dict[str, List[str]], infant_ordering: Dict[str, int], union_isolates: Set[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Uses the previously computed multiple alignments to create a distance matrix between infants.\n",
    "    \n",
    "    For each gene g, a matrix is computed:\n",
    "    d[g](i, j) = min_{x \\in isolates_i, y \\in isolates_j} HAMMING_[gene=g](x, y)\n",
    "\n",
    "    d[g](i, j) > 0 indicates that the gene g \"cuts\"/\"distinguishes\" the isolates of i versus the isolates of j, in the sense that \n",
    "    d[g] is a lower bound on the distance between any isolate of i and any isolate of j.\n",
    "    \"\"\"\n",
    "    aln_path = INFANT_ISOLATE_INDEX.parent / 'gene_alignments' / f'{gene_name}.aln.fasta'\n",
    "    if not aln_path.exists():\n",
    "        raise FileNotFoundError(f\"Multiple alignment for {gene_name} didn't run.\")\n",
    "\n",
    "    # Parse the multiple alignment fasta file.\n",
    "    l_aln = len(next(iter(SeqIO.parse(aln_path, \"fasta\"))).seq)\n",
    "    aligned_seqs = {acc: Seq('-' * l_aln) for acc in union_isolates}  # the aligned strings per isolate.\n",
    "    for record in SeqIO.parse(aln_path, \"fasta\"):\n",
    "        record_acc, contig_idx, _ = record.id.split(\":\")  # third token is assumed to be the gene name (see the cell that runs pgap).\n",
    "        aligned_seqs[record_acc] = record.seq\n",
    "\n",
    "    # compute the distances.\n",
    "    n = len(infant_ordering)\n",
    "    d = np.zeros(shape=(n, n), dtype=int)\n",
    "    for (i, i_idx), (j, j_idx) in itertools.combinations(\n",
    "        infant_ordering.items(),\n",
    "        r=2\n",
    "    ):\n",
    "        i_isolates = infant_groupings[i]\n",
    "        j_isolates = infant_groupings[j]\n",
    "        dist = np.min([hamming(aligned_seqs[x], aligned_seqs[y]) for x, y in itertools.product(i_isolates, j_isolates)])\n",
    "        d[i_idx, j_idx] = dist\n",
    "        d[j_idx, i_idx] = dist\n",
    "    return d\n",
    "\n",
    "\n",
    "def hamming(x: Seq, y: Seq) -> int:\n",
    "    assert len(x) == len(y)\n",
    "    return sum(1 for i, j in zip(x, y) if i != j)\n",
    "\n",
    "\n",
    "# ================================ parse the clustering using the previous set of genes\n",
    "prelim_cluster_df_entries = []\n",
    "with open(\"/mnt/e/infant_nt/database/chronostrain_files_with_REFSEQ/efaecalis.clusters_without_isolate_genes.txt\", \"rt\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        tokens = line.rstrip().split(\"\\t\")\n",
    "        rep = tokens[0]\n",
    "        members = tokens[1].split(\",\")\n",
    "        for member in members:\n",
    "            if member.startswith(\"GCA\"):  # only include isolates\n",
    "                prelim_cluster_df_entries.append({'Accession': member, 'Cluster': rep})\n",
    "\n",
    "prelim_cluster_df = pd.DataFrame(prelim_cluster_df_entries)\n",
    "del prelim_cluster_df_entries\n",
    "    \n",
    "\n",
    "# ================================= go through each cluster one by one and pick gene(s) that splits it.\n",
    "target_gene_set = set()\n",
    "n_clusters = len(pd.unique(prelim_cluster_df['Cluster']))\n",
    "pbar = tqdm(prelim_cluster_df.groupby(\"Cluster\").count().sort_values('Accession', ascending=False).iterrows(), total=n_clusters)\n",
    "for cluster_id, row in pbar:\n",
    "    pbar.set_postfix({\"Cluster\": cluster_id, 'Genes_found': len(target_gene_set)})\n",
    "    section = prelim_cluster_df.loc[prelim_cluster_df['Cluster'] == cluster_id]\n",
    "\n",
    "    infant_groupings = {\n",
    "        infant_id: [\n",
    "            row['Accession']\n",
    "            for _, row in infant_isolate_section.iterrows()\n",
    "        ]\n",
    "        for infant_id, infant_isolate_section in section.merge(infant_isolate_df, on='Accession').groupby(\"Infant\")\n",
    "    }\n",
    "    if len(infant_groupings) < 2:\n",
    "        continue\n",
    "\n",
    "    infant_ordering = {infant_id: idx for idx, infant_id in enumerate(infant_groupings.keys())}\n",
    "    target_genes = get_splitting_genes(infant_groupings, infant_ordering)\n",
    "    target_gene_set = target_gene_set.union(set(target_genes))\n",
    "    # break  # debug; delete to run the whole thing.\n",
    "print(\"Target genes: {}\".format(len(target_gene_set)))\n",
    "\n",
    "# save this to disk.\n",
    "with open(MARKER_SEED_DIR / \"selected_isolate_splitting_genes.txt\", \"wt\") as f:\n",
    "    for g in target_gene_set:\n",
    "        print(g, file=f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "08fc678b-7a46-49b2-abfb-25c310427499",
   "metadata": {},
   "source": [
    "Finally, write these genes to a TSV file for including into the collection of seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45d722fa-7dab-4730-8e58-e4bfa8e2875c",
   "metadata": {},
   "source": [
    "isolate_annotated_tsv = MARKER_SEED_DIR / \"isolate_annotated_seeds.tsv\"\n",
    "with open(isolate_annotated_tsv, \"wt\") as metadata_tsv:\n",
    "    for gene_name in target_gene_set:\n",
    "        if gene_name == \"galE\" or gene_name == \"ssb\":  # these two genes have isolate assemblies with N's inside them. ChronoStrain doesn't handle these gracefully yet, so let's leave them out.\n",
    "            continue\n",
    "        gene_fasta_path = INFANT_ISOLATE_INDEX.parent / 'genes' / f'{gene_name}.fasta'\n",
    "        print(\n",
    "            \"{}\\t{}\\t{}\".format(\n",
    "                gene_name, gene_fasta_path, f\"ISOLATE_ANNOT\"\n",
    "            ), \n",
    "            file=metadata_tsv\n",
    "        )\n",
    "print(f\"Wrote to {isolate_annotated_tsv}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "feb4b1d1-f33a-476e-bc11-3d11e596914b",
   "metadata": {},
   "source": [
    "#### 3.5 Combine marker seed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5942cb82-efea-4c0c-99e5-cd601ca98852",
   "metadata": {},
   "source": [
    "!cat \"$MARKER_SEED_DIR\"/mlst_seeds.tsv > $MARKER_SEED_INDEX\n",
    "!cat \"$MARKER_SEED_DIR\"/manual_seeds.tsv >> $MARKER_SEED_INDEX\n",
    "!cat \"$MARKER_SEED_DIR\"/isolate_annotated_seeds.tsv >> $MARKER_SEED_INDEX\n",
    "\n",
    "print(\"Created Marker seed index: {}\".format(MARKER_SEED_INDEX))\n",
    "assert MARKER_SEED_INDEX.exists()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8e13da06-b7af-43ad-acf6-820b48e08520",
   "metadata": {},
   "source": [
    "### Step 4: Run Chronostrain's make-db command.\n",
    "\n",
    "By the end of the previous step, we have:\n",
    "\n",
    "1) FASTA files for each gene, listing out seed sequence(s).\n",
    "2) A TSV file (marker_seed_index.tsv) containing a list of gene names and the paths to each of these FASTA files.\n",
    "\n",
    "Using these as inputs, we now construct the database files:\n",
    "1) A JSON file of the strain records and their markers.\n",
    "2) A TXT file of strain records clustered by similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63a29b9f-4556-43c0-9aae-501917058d5a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!mkdir -p \"$BLAST_DB_DIR\"\n",
    "!env \\\n",
    "    JAX_PLATFORM_NAME=cpu \\\n",
    "    CHRONOSTRAIN_DB_DIR={CHRONOSTRAIN_DB_DIR} \\\n",
    "    CHRONOSTRAIN_LOG_INI={_cwd}/logging.ini \\\n",
    "    chronostrain -c chronostrain.ini \\\n",
    "        make-db \\\n",
    "        -m $MARKER_SEED_INDEX \\\n",
    "        -r $EUROPEAN_ISOLATE_INDEX \\\n",
    "        -r $REFSEQ_INDEX \\\n",
    "        -r $INFANT_ISOLATE_INDEX \\\n",
    "        -b $BLAST_DB_NAME -bd $BLAST_DB_DIR \\\n",
    "        --min-pct-idty $MIN_PCT_IDTY \\\n",
    "        -o $CHRONOSTRAIN_TARGET_JSON \\\n",
    "        --threads $NUM_CORES"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb366132-063b-4709-8e35-21ca2f0b5555",
   "metadata": {},
   "source": [
    "# Perform clustering\n",
    "thresh = 0.998\n",
    "!env \\\n",
    "    JAX_PLATFORM_NAME=cpu \\\n",
    "    CHRONOSTRAIN_DB_DIR={CHRONOSTRAIN_DB_DIR} \\\n",
    "    CHRONOSTRAIN_LOG_INI={_cwd}/logging.ini \\\n",
    "    chronostrain -c chronostrain.ini \\\n",
    "      cluster-db \\\n",
    "      -i $CHRONOSTRAIN_TARGET_JSON \\\n",
    "      -o $CHRONOSTRAIN_TARGET_CLUSTERS \\\n",
    "      --ident-threshold {thresh}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ed936012-425e-4fe2-a414-6a790951dbc2",
   "metadata": {},
   "source": [
    "# OPTIONAL: Compute some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a6eaaa4-e4e8-4311-a22d-0e74888bb807",
   "metadata": {},
   "source": [
    "from chronostrain.database import JSONParser\n",
    "src_db = JSONParser(\n",
    "    entries_file=CHRONOSTRAIN_TARGET_JSON,\n",
    "    data_dir=CHRONOSTRAIN_DB_DIR,\n",
    "    marker_max_len=50000,  # this parameter is no longer used (todo remove this)\n",
    "    force_refresh=False\n",
    ").parse()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c7048e4-570d-4be7-9503-08548ba39d0f",
   "metadata": {},
   "source": [
    "isolate_index = pd.read_csv(INFANT_ISOLATE_INDEX, sep='\\t')\n",
    "isolate_ids = set(isolate_index['Accession'])\n",
    "n0 = sum(1 for s in src_db.all_strains() if not s.id in isolate_ids)\n",
    "n1 = sum(1 for s in src_db.all_strains() if s.metadata.genus == 'Enterococcus' and s.metadata.species == 'faecalis' and not s.id.startswith(\"GCA\"))\n",
    "n2 = sum(1 for s in src_db.all_strains() if s.metadata.genus == 'Enterococcus' and s.metadata.species == 'faecalis' and s.id.startswith(\"GCA\"))\n",
    "print(\"# of total entries:\", len(src_db.all_strains()))\n",
    "print(\"# of non-infant isolate entries:\", n0)\n",
    "print(\"# of non-infant isolate E. faecalis entries:\", n1)\n",
    "print(\"# of infant isolate E. faecalis entries:\", n2)\n",
    "\n",
    "\n",
    "marker_lens = np.array([sum(len(m) for m in s.markers) for s in src_db.all_strains()])\n",
    "genome_lens = np.array([s.metadata.total_len for s in src_db.all_strains()])\n",
    "ratios = marker_lens / genome_lens\n",
    "print(\"Database's E. faecalis marker fraction of genome: {} [mean]\".format(np.mean(ratios)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0205f9d-38ea-4bf3-8ef2-3940db05e1c8",
   "metadata": {},
   "source": [
    "df_entries = []\n",
    "with open(CHRONOSTRAIN_TARGET_CLUSTERS, \"rt\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        tokens = line.rstrip().split(\"\\t\")\n",
    "        rep = tokens[0]\n",
    "        members = tokens[1].split(\",\")\n",
    "        for member in members:\n",
    "            # if member.startswith(\"GCA\"):  # only include isolates\n",
    "            df_entries.append({'Accession': member, 'Cluster': rep})\n",
    "cluster_df = pd.DataFrame(df_entries)\n",
    "del df_entries\n",
    "print(\"# clusters = {}\".format(len(pd.unique(cluster_df['Cluster']))))\n",
    "\n",
    "\n",
    "n_efaecalis = 0\n",
    "for cluster_id in pd.unique(cluster_df['Cluster']):\n",
    "    s = src_db.get_strain(cluster_id)\n",
    "    if s.metadata.genus == 'Enterococcus' and s.metadata.species == 'faecalis':\n",
    "        n_efaecalis += 1\n",
    "print(\"# efaecalis clusters = {}\".format(n_efaecalis))\n",
    "\n",
    "\n",
    "n_infant_clusters = len(pd.unique(cluster_df.loc[cluster_df['Accession'].str.startswith(\"GCA\"), \"Cluster\"]))\n",
    "print(\"# efaecalis clusters with infant isolates = {}\".format(n_infant_clusters))\n",
    "\n",
    "\n",
    "n_infants_per_cluster = []\n",
    "for cluster_id, section in cluster_df.loc[cluster_df['Accession'].str.startswith(\"GCA\")].merge(infant_isolate_df, on='Accession').groupby(\"Cluster\"):\n",
    "    infant_ids = list(pd.unique(section['Infant']))\n",
    "    n_infants_per_cluster.append(len(infant_ids))\n",
    "    # print(cluster_id, \"->\", infant_ids)\n",
    "print(\"# src infants per cluster containing some isolate [min={}, max={}, mean={}, median={}]\".format(\n",
    "    np.min(n_infants_per_cluster),\n",
    "    np.max(n_infants_per_cluster),\n",
    "    np.mean(n_infants_per_cluster),\n",
    "    np.median(n_infants_per_cluster)\n",
    "))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "86b208fa-739f-46d3-97d4-2898f927aca8",
   "metadata": {},
   "source": [
    "# mSWEEP database (poppunk clustering)\n",
    "\n",
    "To obtain PopPUNK clustering, it was run with the following commands/settings:\n",
    "\n",
    "```\n",
    "poppunk --create-db --output EFaec --r-files input.tsv\n",
    "poppunk --fit-model dbscan --ref-db EFaec --output dbscan\n",
    "poppunk --fit-model refine --ref-db EFaec --model-dir dbscan --output refine --max-a-dist 0.9 --max-pi-dist 0.9\n",
    "```\n",
    "\n",
    "where `input.tsv` is the table of ~2000 european E.faecalis isolates, plus the ~350 infant E.faecalis isolates from ELMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58d0e438-6ea0-46d9-85b3-319f56cf9d9a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "poppunk_clust = pd.read_csv(\"/mnt/e/infant_nt/database/mgems/ref_dir/Efaecalis/ref_clu.tsv\", sep=\"\\t\").rename(columns={\"id\": \"Accession\", \"cluster\": \"Cluster\"})\n",
    "poppunk_clust"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fadf2a78-3b6f-444a-84bb-f0f068331d97",
   "metadata": {},
   "source": [
    "print(\"# efaecalis clusters = {}\".format(len(pd.unique(poppunk_clust['Cluster']))))\n",
    "\n",
    "\n",
    "n_infant_clusters = len(pd.unique(poppunk_clust.loc[poppunk_clust['Accession'].str.startswith(\"GCA\"), \"Cluster\"]))\n",
    "print(\"# efaecalis clusters with infant isolates = {}\".format(n_infant_clusters))\n",
    "\n",
    "\n",
    "n_infants_per_cluster = []\n",
    "for cluster_id, section in poppunk_clust.loc[poppunk_clust['Accession'].str.startswith(\"GCA\")].merge(infant_isolate_df, on='Accession').groupby(\"Cluster\"):\n",
    "    infant_ids = list(pd.unique(section['Infant']))\n",
    "    n_infants_per_cluster.append(len(infant_ids))\n",
    "    # print(cluster_id, \"->\", infant_ids)\n",
    "\n",
    "print(\"# src infants per cluster containing some isolate [min={}, max={}, mean={}, median={}]\".format(\n",
    "    np.min(n_infants_per_cluster),\n",
    "    np.max(n_infants_per_cluster),\n",
    "    np.mean(n_infants_per_cluster),\n",
    "    np.median(n_infants_per_cluster)\n",
    "))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronostrain",
   "language": "python",
   "name": "chronostrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
